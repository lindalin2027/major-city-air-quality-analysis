{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01d745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from openaq import OpenAQ\n",
    "\n",
    "api_key = os.getenv(\"OPENAQ_API_KEY\")\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAQ(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ff943",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "location_response = client.locations.get(1884)\n",
    "sensors = location_response.results[0].sensors\n",
    "\n",
    "for sensor in sensors:\n",
    "\n",
    "    sensor_id = sensor.id\n",
    "    parameter_name = sensor.parameter.name\n",
    "    \n",
    "    print(f\"\\nFetching data for sensor {sensor_id} ({parameter_name})...\")\n",
    "    \n",
    "    # Use daily averages to reduce data volume\n",
    "    page = 1\n",
    "    \n",
    "    while True: # Keep looping until we get all data\n",
    "        try:\n",
    "            # Fetch daily measurements with date range\n",
    "            response = client.measurements.list(\n",
    "                sensors_id=sensor_id,\n",
    "                datetime_from=\"2020-01-01T00:00:00Z\",\n",
    "                datetime_to=\"2025-01-01T00:00:00Z\",\n",
    "                data=\"days\",  # Daily averages\n",
    "                limit=1000,\n",
    "                page=page # Request page 1, then page 2, then page 3... for each loop\n",
    "            )\n",
    "            \n",
    "            if not response.results:\n",
    "                break\n",
    "            \n",
    "            # Convert to DataFrame-friendly format\n",
    "            for result in response.results:\n",
    "                all_data.append({\n",
    "                    'sensor_id': sensor_id,\n",
    "                    'parameter': parameter_name,\n",
    "                    'datetime_utc': result.period.datetime_from.utc,\n",
    "                    'datetime_local': result.period.datetime_from.local,\n",
    "                    'value': result.value,\n",
    "                    'units': result.parameter.units,\n",
    "                    'coverage_percent': result.coverage.percent_complete if result.coverage else None,\n",
    "                    'min': result.summary.min if result.summary else None,\n",
    "                    'max': result.summary.max if result.summary else None,\n",
    "                    'median': result.summary.median if result.summary else None,\n",
    "                })\n",
    "            \n",
    "            print(f\"  Page {page}: {len(response.results)} records\")\n",
    "            \n",
    "            # Check if there are more pages\n",
    "            if len(response.results) < 1000: # Got fewer than 1000, must be the last page\n",
    "                break\n",
    "            \n",
    "            page += 1 # Next page\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "# Close the client\n",
    "client.close()\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa3cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Processing sensor 1671\n",
      "======================================================================\n",
      "\n",
      "Fetching data for sensor 1671...\n",
      "  Page 1: 1000 records\n",
      "  Page 2: 1000 records\n",
      "  Page 3: 87 records\n",
      "✓ Collected 2087 records for sensor 1671\n",
      "✓ Saved: sensor_data/sensor_1671_data.csv (2087 records)\n",
      "\n",
      "[2/5] Processing sensor 1404\n",
      "======================================================================\n",
      "\n",
      "Fetching data for sensor 1404...\n",
      "  Page 1: 1000 records\n",
      "  Page 2: 1000 records\n",
      "  Page 3: 829 records\n",
      "✓ Collected 2829 records for sensor 1404\n",
      "✓ Saved: sensor_data/sensor_1404_data.csv (2829 records)\n",
      "\n",
      "[3/5] Processing sensor 564\n",
      "======================================================================\n",
      "\n",
      "Fetching data for sensor 564...\n",
      "  Page 1: 473 records\n",
      "✓ Collected 473 records for sensor 564\n",
      "✓ Saved: sensor_data/sensor_564_data.csv (473 records)\n",
      "\n",
      "[4/5] Processing sensor 8330\n",
      "======================================================================\n",
      "\n",
      "Fetching data for sensor 8330...\n",
      "  Page 1: 695 records\n",
      "✓ Collected 695 records for sensor 8330\n",
      "✓ Saved: sensor_data/sensor_8330_data.csv (695 records)\n",
      "\n",
      "[5/5] Processing sensor 2183\n",
      "======================================================================\n",
      "\n",
      "Fetching data for sensor 2183...\n",
      "  Page 1: 1000 records\n",
      "  Page 2: 1000 records\n",
      "  Page 3: 1000 records\n",
      "  Page 4: 92 records\n",
      "✓ Collected 3092 records for sensor 2183\n",
      "✓ Saved: sensor_data/sensor_2183_data.csv (3092 records)\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Successful: 5\n",
      "Failed: 0\n",
      "Total: 5\n"
     ]
    }
   ],
   "source": [
    "# this version saves all sensors' data into one CSV, accepts fuzzy date formats\n",
    "from openaq import OpenAQ\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dateutil import parser  \n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAQ_API_KEY\")\n",
    "client = OpenAQ(api_key=api_key)\n",
    "\n",
    "def parse_date_to_openaq_format(date_input):\n",
    "    \"\"\"\n",
    "    Convert various date formats to OpenAQ API format (ISO 8601 with Z).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    date_input : str, datetime, or None\n",
    "        Date in various formats:\n",
    "        - \"1/1/2020\", \"01/01/2020\", \"2020-1-1\"\n",
    "        - \"January 1, 2020\", \"Jan 1 2020\"\n",
    "        - \"2020-01-01\", \"2020/01/01\"\n",
    "        - datetime object\n",
    "        - None (returns None)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Date in format \"YYYY-MM-DDTHH:MM:SSZ\"\n",
    "        \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> parse_date_to_openaq_format(\"1/1/2020\")\n",
    "    '2020-01-01T00:00:00Z'\n",
    "    \n",
    "    >>> parse_date_to_openaq_format(\"January 15, 2023\")\n",
    "    '2023-01-15T00:00:00Z'\n",
    "    \n",
    "    >>> parse_date_to_openaq_format(\"2024-12-31\")\n",
    "    '2024-12-31T00:00:00Z'\n",
    "    \"\"\"\n",
    "    if date_input is None:\n",
    "        return None\n",
    "    \n",
    "    # If already a datetime object\n",
    "    if isinstance(date_input, datetime):\n",
    "        return date_input.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    # If already in correct format, return as-is\n",
    "    if isinstance(date_input, str) and date_input.endswith('Z'):\n",
    "        return date_input\n",
    "    \n",
    "    try:\n",
    "        # Use dateutil.parser to handle various formats\n",
    "        # dayfirst=False means 1/2/2020 = Jan 2, not Feb 1 (US format)\n",
    "        dt = parser.parse(date_input, dayfirst=False)\n",
    "        return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse date '{date_input}'. Error: {e}\")\n",
    "\n",
    "def get_sensor_data(client, sensor_id, datetime_from=\"2020-01-01T00:00:00Z\", datetime_to=\"2025-01-01T00:00:00Z\"):\n",
    "    \"\"\"\n",
    "    Fetch data for a single sensor.\n",
    "    \"\"\"\n",
    "    datetime_from = parse_date_to_openaq_format(datetime_from)\n",
    "    datetime_to = parse_date_to_openaq_format(datetime_to)\n",
    "\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    \n",
    "    print(f\"\\nFetching data for sensor {sensor_id}...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = client.measurements.list(\n",
    "                sensors_id=sensor_id,\n",
    "                datetime_from=datetime_from,  \n",
    "                datetime_to=datetime_to,      \n",
    "                data=\"days\",\n",
    "                limit=1000,\n",
    "                page=page\n",
    "            )\n",
    "            \n",
    "            if not response.results:\n",
    "                break\n",
    "            \n",
    "            for result in response.results:\n",
    "                all_data.append({\n",
    "                    'sensor_id': sensor_id,\n",
    "                    'parameter': result.parameter.name,\n",
    "                    'datetime_utc': result.period.datetime_from.utc,\n",
    "                    'datetime_local': result.period.datetime_from.local,\n",
    "                    'value': result.value,\n",
    "                    'units': result.parameter.units,\n",
    "                    'coverage_percent': result.coverage.percent_complete if result.coverage else None,\n",
    "                    'min': result.summary.min if result.summary else None,\n",
    "                    'max': result.summary.max if result.summary else None,\n",
    "                    'median': result.summary.median if result.summary else None,\n",
    "                })\n",
    "            \n",
    "            print(f\"  Page {page}: {len(response.results)} records\")\n",
    "            \n",
    "            if len(response.results) < 1000:\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    print(f\"Collected {len(df)} records for sensor {sensor_id}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Your list of sensor IDs\n",
    "    sensor_ids = [1671, 1404, 564, 8330, 2183]\n",
    "    \n",
    "    os.makedirs('sensor_data', exist_ok=True)\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for idx, sensor_id in enumerate(sensor_ids, 1):\n",
    "        print(f\"\\n[{idx}/{len(sensor_ids)}] Processing sensor {sensor_id}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            df = get_sensor_data(client, sensor_id)\n",
    "            \n",
    "            if not df.empty:\n",
    "                output_file = f\"sensor_data/sensor_{sensor_id}_data.csv\"\n",
    "                df.to_csv(output_file, index=False)\n",
    "                print(f\"Saved: {output_file} ({len(df)} records)\")\n",
    "                successful += 1\n",
    "            else:\n",
    "                print(f\"No data for sensor {sensor_id}\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sensor {sensor_id}: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Total: {len(sensor_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e40f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
